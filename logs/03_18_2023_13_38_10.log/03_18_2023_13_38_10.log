[ 2023-03-18 13:38:18,680 ] 26 root - INFO - Entered the data ingestion method or components
[ 2023-03-18 13:38:18,685 ] 29 root - INFO - Exported Read The Dataset as DataFrame
[ 2023-03-18 13:38:18,699 ] 35 root - INFO - Train Test Split Inititated
[ 2023-03-18 13:38:18,709 ] 41 root - INFO - Ingestion of Data is completed
[ 2023-03-18 13:38:18,709 ] 62 root - INFO - Train Data : artifact\train.csv, Test Data : artifact\test.csv
[ 2023-03-18 13:38:18,716 ] 85 root - INFO - Read train and test data completed
[ 2023-03-18 13:38:18,716 ] 87 root - INFO - Obtaining preprocessing object
[ 2023-03-18 13:38:18,716 ] 34 root - INFO - Entered the data transfomation method or components
[ 2023-03-18 13:38:18,716 ] 44 root - INFO - Categorical columns: ['gender', 'race_ethnicity', 'parental_level_of_education', 'lunch', 'test_preparation_course']
[ 2023-03-18 13:38:18,716 ] 45 root - INFO - Numerical columns: ['writing_score', 'reading_score']
[ 2023-03-18 13:38:18,739 ] 61 root - INFO - Categorical Pipeline: Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),
                ('one_hot_encoder', OneHotEncoder()),
                ('scaler', StandardScaler(with_mean=False))])
[ 2023-03-18 13:38:18,740 ] 62 root - INFO - Numerical Pipline: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler(with_mean=False))])
[ 2023-03-18 13:38:18,740 ] 71 root - INFO - Categorical Pipline &Numerical Pipeline Excueted Scuceesully
[ 2023-03-18 13:38:18,750 ] 72 root - INFO - Preprocesssor is returned ColumnTransformer(transformers=[('num_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='median')),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['writing_score', 'reading_score']),
                                ('cat_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='most_frequent')),
                                                 ('one_hot_encoder',
                                                  OneHotEncoder()),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['gender', 'race_ethnicity',
                                  'parental_level_of_education', 'lunch',
                                  'test_preparation_course'])])
[ 2023-03-18 13:38:18,750 ] 103 root - INFO - Categorical columns: ['gender', 'race_ethnicity', 'parental_level_of_education', 'lunch', 'test_preparation_course']
[ 2023-03-18 13:38:18,750 ] 104 root - INFO - Numerical columns: ['writing_score', 'reading_score']
[ 2023-03-18 13:38:18,750 ] 105 root - INFO - Target columns: math_score
[ 2023-03-18 13:38:18,750 ] 107 root - INFO - Removing Target Columns from Train and Test Set
[ 2023-03-18 13:38:18,751 ] 116 root - INFO - Applying preprocessing object on training dataframe and testing dataframe.
[ 2023-03-18 13:38:18,768 ] 131 root - INFO - Data Transformation Complted.
[ 2023-03-18 13:38:18,768 ] 15 root - INFO - Saving object {obj} on file: {file_path} Started.
[ 2023-03-18 13:38:18,768 ] 18 root - INFO - Making Directory :artifact
[ 2023-03-18 13:38:18,768 ] 20 root - INFO - Directory Made
[ 2023-03-18 13:38:18,779 ] 21 root - INFO - Dumping object : ColumnTransformer(transformers=[('num_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='median')),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['writing_score', 'reading_score']),
                                ('cat_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='most_frequent')),
                                                 ('one_hot_encoder',
                                                  OneHotEncoder()),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['gender', 'race_ethnicity',
                                  'parental_level_of_education', 'lunch',
                                  'test_preparation_course'])])
[ 2023-03-18 13:38:18,797 ] 25 root - INFO - Dumping of obj ColumnTransformer(transformers=[('num_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='median')),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['writing_score', 'reading_score']),
                                ('cat_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='most_frequent')),
                                                 ('one_hot_encoder',
                                                  OneHotEncoder()),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['gender', 'race_ethnicity',
                                  'parental_level_of_education', 'lunch',
                                  'test_preparation_course'])]) Successfull <_io.BufferedWriter name='artifact\\preprocessor.pkl'>
[ 2023-03-18 13:38:18,797 ] 141 root - INFO - Saved preprocessing object as Pickle File.
[ 2023-03-18 13:38:18,798 ] 66 root - INFO - Train Array : [[ 4.30533129  4.62062258  0.         ...  0.          2.09830697
  73.        ]
 [ 4.37056358  4.75652325  2.0025047  ...  0.          2.09830697
  58.        ]
 [ 2.80498857  3.12571528  0.         ...  0.          2.09830697
  55.        ]
 ...
 [ 4.56626046  4.96037424  2.0025047  ...  0.          2.09830697
  62.        ]
 [ 5.47951255  5.36807623  2.0025047  ...  2.09830697  0.
  67.        ]
 [ 3.71824066  4.34882125  0.         ...  0.          2.09830697
  73.        ]],Test Array : [[ 3.00068544  3.32956627  0.         ...  2.09830697  0.
  52.        ]
 [ 3.32684691  4.00906959  2.0025047  ...  0.          2.09830697
  40.        ]
 [ 4.56626046  4.68857291  2.0025047  ...  0.          2.09830697
  65.        ]
 ...
 [ 5.21858338  5.23217557  0.         ...  2.09830697  0.
  78.        ]
 [ 3.78347295  3.66931793  2.0025047  ...  2.09830697  0.
  46.        ]
 [ 4.1748667   3.94111926  2.0025047  ...  2.09830697  0.
  57.        ]],Preprocessor Array : artifact\preprocessor.pkl
[ 2023-03-18 13:38:18,798 ] 38 root - INFO - Entered the Model Traineer method or components
[ 2023-03-18 13:38:18,798 ] 40 root - INFO - Split training and test input data
[ 2023-03-18 13:38:18,799 ] 31 root - INFO - Evaluting Model Started
[ 2023-03-18 13:38:18,799 ] 39 root - INFO - Evaluting Model: RandomForestRegressor() on Params; {'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], 'max_features': ['sqrt', 'log2', None], 'n_estimators': [8, 16, 32, 64, 128, 256]}
[ 2023-03-18 13:38:18,799 ] 43 root - INFO - GridSearchCV(cv=3, estimator=RandomForestRegressor(),
             param_grid={'criterion': ['squared_error', 'friedman_mse',
                                       'absolute_error', 'poisson'],
                         'max_features': ['sqrt', 'log2', None],
                         'n_estimators': [8, 16, 32, 64, 128, 256]})
[ 2023-03-18 13:40:12,997 ] 45 root - INFO - Doing Grid Search CV
[ 2023-03-18 13:40:22,538 ] 50 root - INFO - Model Trained With Hyper Parameter Tunning
[ 2023-03-18 13:40:22,642 ] 57 root - INFO - Real Train Data [ 73.  58.  55.  37.  74.  79.  53.  75.  47.  66.  53.  66.  38.  64.
  57.  85.  90.  68.  76.  64.  62.  62.  76.  63.  54.  81.  66.  62.
  67.  74.  77.  85.  55.  52.  62.  74.  38.  48.  92.  76.  50.  48.
  50.  75.  45.  75.  60.  73.  81.  79.  75.  57.  79.  57.  68.  70.
  59.  47.  59.  68.  65.  58.  86.  76.  57.  67.  80.  61.  62.  65.
  39.  81.  81.  79.  91.  74.  75.  61.  54.  52.  66.  81.  97.  53.
  74.  57.  38.  69.  69.  84.  91.  36.  72.  79.  53.  65.  83.  87.
  47.  59.  68.  52.  58.  68.  64.  35.  69.  73.  79.  65.  69.  29.
  65.  80.  42.  44.  65.  73.  53.  65.  72.  80.  71.  63.  53.  83.
  71.  62.  59.  72.  62.  46.  67.  85.  91.  59.  76.  66.  59.  79.
  98.  27.  73.  88.  73.  68.  69.  72.  62.  86.  69.  82.  87.  58.
  89.  84.  35.  77.  51.  63.  84.  64.  54.  42.  56.  94.  80.  70.
  49.  69.   8.  58.  73.  61.  47.  59.  65.  26.  64.  59.  76.  82.
  77.  91.  69.  74.  62.  69.  73.  69.  62.  80.  48.  86.  43.  50.
  49.  65.  55.  51.  73.  52.  43.  75.  44.  81.  64.  65.  71.  65.
  30.  83.  85.  66.  76.  58.  92.  37.  75.  60.  55.  66.  76.  96.
  50.  49.  83.  92.  96.  73.  82.  67.  67.  48.  22.  70.  77.  56.
  53.  65.  58.  89.  66.  74.  83.  62.  73.  81.  47.  87.  80.  62.
  53. 100.  63.   0.  85.  68.  61.  34.  58.  78.  70.  61.  79.  68.
  70.  51.  73.  74.  82.  62.  19.  67.  70.  87.  63.  65.  61.  62.
  65.  49.  54.  46.  50.  45.  86.  67.  64.  55.  64.  71.  80.  71.
  66.  82.  50.  33.  40.  70.  63.  86.  48.  92.  59.  65.  62.  93.
  53.  58.  71.  77.  74.  78.  71.  46.  53.  62.  64.  62.  88.  62.
  69.  90.  53.  88.  69.  81.  65.  61.  99.  71.  62.  52.  79.  90.
  72.  61.  63.  89.  35.  54.  74.  78.  76.  53.  68.  75.  67.  69.
  81.  63.  41.  57.  61.  84.  61.  59.  49.  49.  41.  58.  76.  76.
  53.  49.  48.  55.  32.  61.  68.  72.  44.  63.  60.  90.  77.  61.
  58.  49.  64.  98.  55.  94.  71.  53.  63.  55.  46.  63.  76.  58.
  90.  72.  76.  32.  64.  36.  61.  81.  55.  85.  60.  60.  59.  80.
  82.  50.  35.  52.  96.  77.  82.  71.  83.  85.  69.  62.  69.  44.
  50.  54.  67.  59.  47.  54.  80.  61.  64.  49.  62.  44.  89.  81.
  68.  77.  65.  77.  88.  83.  73.  69.  68.  69.  58.  46.  61.  57.
  49.  61.  49.  56.  75.  58.  63.  78.  41.  46.  54.  73.  64. 100.
  57.  50.  67.  49.  70.  84.  42.  75.  61.  59.  99.  78.  78.  40.
  74.  74.  82.  62.  58.  75.  63.  34.  69.  52.  63.  69. 100.  75.
  68.  58.  65.  94.  76.  84.  65.  72.  63.  76.  69.  78.  81. 100.
  70.  44.  30.  84.  68.  84.  50.  81.  70.  84.  59.  98.  67.  73.
  88.  50.  77.  40.  75.  77.  54.  90.  64.  41.  49.  85.  77.  57.
  39.  94.  71. 100.  71.  58.  45.  67.  81.  82.  71.  70.  51.  66.
  79.  50.  72.  63.  84.  59.  91.  47.  97.  65.  58.  63.  54.  60.
  39.  68.  58.  57.  82.  91.  65.  57.  99.  56.  70.  59.  52.  82.
  53.  70.  67.  71.  50.  74.  66.  47.  59.  66.  52.  81.  75.  90.
  69.  82.  65.  62.  62.  62.  28.  61.  40.  52.  72.  45.  68.  94.
  44.  55.  74.  71.  79.  80.  46.  41.  52.  48.  77.  55.  45.  65.
  71.  65.  49.  45.  75.  69.  60.  81.  42.  68.  75.  62.  64.  82.
  74.  66.  81.  40.  60.  75.  49.  61.  87.  79.  87.  53.  77.  59.
  76.  52.  95.  91.  54.  55.  65.  93.  53.  67.  53.  68.  71.  41.
  57.  45.  88.  60.  81.  77.  66.  49.  53.  61.  47.  89.  71.  94.
  59.  59.  58.  85.  74.  65.  54.  59.  66.  77.  71.  93.  55.  52.
  73.  71.  60.  69.  40.  76.  52.  47.  77.  45.  63.  51.  48.  60.
  59.  72.  55.  76.  73.  72.  56.  82.  63.  67.  62.  87.  44.  55.
  66.  63.  85.  57.  60.  88.  53.  87.  64.  77.  67.  65.  50.  40.
  57.  57.  23.  67.  79.  81.  77.  70.  78.  51.  54.  63.  35.  64.
  67.  92.  65.  46.  46.  37.  80.  42.  68.  58.  45.  75.  80.  49.
  74.  67.  61.  63.  77.  60.  56.  77.  60.  73.  73.  78.  73.  73.
  87.  62.  57.  46.  74.  92.  61.  81.  73.  72.  66.  73.  32.  82.
  68.  97.  51.  59. 100.  87.  62.  69.  51.  88.  58.  59.  61.  62.
  67.  73.] and Predicted Train Data [53.0859375  46.859375   61.32421875 58.12109375 91.93359375 62.19140625
 92.33789062 59.046875   83.3125     48.75       51.57421875 83.51171875
 60.02734375 75.61328125 48.5625     50.40625    69.75       61.41015625
 72.69140625 56.8984375  70.28515625 59.296875   58.59375    58.08203125
 80.46875    70.51171875 59.578125   92.89453125 69.12695312 51.03515625
 56.4921875  83.62890625 69.86328125 84.99609375 74.9765625  69.265625
 60.890625   62.078125   69.4296875  39.75976562 57.4921875  51.16796875
 50.796875   84.265625   66.25       49.59765625 68.765625   95.828125
 59.10546875 50.171875   72.49609375 89.71679688 45.125      85.36328125
 86.0859375  71.33203125 53.34375    86.74023438 73.16015625 64.4296875
 56.92578125 71.59765625 72.421875   68.296875   51.3359375  79.91015625
 69.52539062 54.25       92.42382812 50.84765625 59.421875   78.6328125
 52.18359375 63.4921875  61.84375    96.26757812 84.6015625  67.75390625
 62.71484375 91.34375    83.5234375  77.11328125 41.6484375  83.96875
 51.18359375 59.46875    86.76367188 79.9296875  64.90625    79.53515625
 80.94140625 75.94140625 52.1484375  69.1796875  77.796875   66.7734375
 83.04882812 67.76171875 50.0078125  65.2109375  51.8125     56.28515625
 70.23242188 76.97265625 93.60742188 84.06445312 54.41015625 84.83984375
 62.76953125 82.6953125  22.46484375 55.7421875  57.2109375  74.6328125
 78.0703125  55.5703125  24.4140625  64.953125   70.3671875  62.46875
 84.203125   65.7734375  73.4609375  51.91015625 70.74023438 85.09765625
 77.546875   61.13671875 59.48046875 72.5        51.625      48.140625
 66.68164062 61.0078125  78.6015625  88.546875   46.56640625 63.07226562
 61.515625   76.796875   69.9140625  95.58203125 65.5078125  91.7265625
 65.07421875 56.3359375  90.03515625 63.37109375 45.890625   90.72265625
 89.48828125 52.33984375 79.109375   71.578125   61.44921875 53.125
 68.7421875  72.76953125 70.671875   75.85546875 80.28125    69.73828125
 57.953125   62.44921875 62.640625   73.45703125 66.89648438 57.2265625
 69.77539062 75.98828125 73.0546875  55.60546875 80.62109375 81.44921875
 80.6484375  72.859375   53.328125   57.2734375  84.01171875 76.15625
 89.671875   70.92578125 64.06640625 64.8515625  77.76367188 33.51757812
 65.10546875 86.11523438 71.45703125 62.546875   54.71484375 65.390625
 52.515625   91.32226562 75.8125     62.98046875 77.36914062 80.11328125
 48.859375   54.15625   ]
[ 2023-03-18 13:40:22,644 ] 58 root - INFO - Real Train Data [ 52.  40.  65.  63.  93.  66.  84.  65.  86.  51.  50.  76.  53.  79.
  43.  57.  62.  51.  74.  54.  65.  53.  51.  68.  79.  67.  60.  97.
  68.  59.  67.  85.  71.  88.  85.  74.  62.  55.  74.  37.  68.  53.
  63.  87.  65.  43.  69.  91.  59.  59.  67.  79.  40.  87.  86.  71.
  65.  82.  69.  71.  62.  72.  66.  72.  44.  82.  66.  42.  97.  59.
  55.  71.  59.  64.  55. 100.  88.  62.  64.  88.  95.  75.  29.  88.
  39.  66.  85.  80.  72.  79.  79.  80.  52.  68.  75.  66.  78.  74.
  61.  69.  40.  48.  72.  79.  87.  88.  54.  81.  68.  78.  24.  54.
  63.  69.  88.  54.  18.  67.  61.  61.  87.  65.  76.  48.  65.  82.
  74.  68.  53.  71.  47.  61.  67.  58.  69.  87.  43.  70.  66.  88.
  73.  91.  69.  90.  60.  59.  97.  77.  27.  94.  85.  56.  77.  72.
  56.  64.  80.  81.  79.  70.  79.  73.  56.  66.  59.  78.  63.  58.
  69.  83.  70.  54.  86.  80.  79.  69.  58.  52.  78.  76.  77.  74.
  70.  74.  80.  29.  73.  87.  59.  62.  52.  71.  48.  89.  65.  67.
  75.  78.  46.  57.] and Predicted Train Data [53.0859375  46.859375   61.32421875 58.12109375 91.93359375 62.19140625
 92.33789062 59.046875   83.3125     48.75       51.57421875 83.51171875
 60.02734375 75.61328125 48.5625     50.40625    69.75       61.41015625
 72.69140625 56.8984375  70.28515625 59.296875   58.59375    58.08203125
 80.46875    70.51171875 59.578125   92.89453125 69.12695312 51.03515625
 56.4921875  83.62890625 69.86328125 84.99609375 74.9765625  69.265625
 60.890625   62.078125   69.4296875  39.75976562 57.4921875  51.16796875
 50.796875   84.265625   66.25       49.59765625 68.765625   95.828125
 59.10546875 50.171875   72.49609375 89.71679688 45.125      85.36328125
 86.0859375  71.33203125 53.34375    86.74023438 73.16015625 64.4296875
 56.92578125 71.59765625 72.421875   68.296875   51.3359375  79.91015625
 69.52539062 54.25       92.42382812 50.84765625 59.421875   78.6328125
 52.18359375 63.4921875  61.84375    96.26757812 84.6015625  67.75390625
 62.71484375 91.34375    83.5234375  77.11328125 41.6484375  83.96875
 51.18359375 59.46875    86.76367188 79.9296875  64.90625    79.53515625
 80.94140625 75.94140625 52.1484375  69.1796875  77.796875   66.7734375
 83.04882812 67.76171875 50.0078125  65.2109375  51.8125     56.28515625
 70.23242188 76.97265625 93.60742188 84.06445312 54.41015625 84.83984375
 62.76953125 82.6953125  22.46484375 55.7421875  57.2109375  74.6328125
 78.0703125  55.5703125  24.4140625  64.953125   70.3671875  62.46875
 84.203125   65.7734375  73.4609375  51.91015625 70.74023438 85.09765625
 77.546875   61.13671875 59.48046875 72.5        51.625      48.140625
 66.68164062 61.0078125  78.6015625  88.546875   46.56640625 63.07226562
 61.515625   76.796875   69.9140625  95.58203125 65.5078125  91.7265625
 65.07421875 56.3359375  90.03515625 63.37109375 45.890625   90.72265625
 89.48828125 52.33984375 79.109375   71.578125   61.44921875 53.125
 68.7421875  72.76953125 70.671875   75.85546875 80.28125    69.73828125
 57.953125   62.44921875 62.640625   73.45703125 66.89648438 57.2265625
 69.77539062 75.98828125 73.0546875  55.60546875 80.62109375 81.44921875
 80.6484375  72.859375   53.328125   57.2734375  84.01171875 76.15625
 89.671875   70.92578125 64.06640625 64.8515625  77.76367188 33.51757812
 65.10546875 86.11523438 71.45703125 62.546875   54.71484375 65.390625
 52.515625   91.32226562 75.8125     62.98046875 77.36914062 80.11328125
 48.859375   54.15625   ]
[ 2023-03-18 13:40:22,645 ] 65 root - INFO - Test model score : 0.9755365603604522 
 Test Model Score: 0.8448045870912733
[ 2023-03-18 13:40:22,645 ] 67 root - INFO - Report Addded : {'Random Forest': 0.8448045870912733}
[ 2023-03-18 13:40:22,645 ] 39 root - INFO - Evaluting Model: DecisionTreeRegressor() on Params; {'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], 'splitter': ['best', 'random'], 'max_features': ['sqrt', 'log2']}
[ 2023-03-18 13:40:22,647 ] 43 root - INFO - GridSearchCV(cv=3, estimator=DecisionTreeRegressor(),
             param_grid={'criterion': ['squared_error', 'friedman_mse',
                                       'absolute_error', 'poisson'],
                         'max_features': ['sqrt', 'log2'],
                         'splitter': ['best', 'random']})
[ 2023-03-18 13:40:22,927 ] 45 root - INFO - Doing Grid Search CV
[ 2023-03-18 13:40:22,940 ] 50 root - INFO - Model Trained With Hyper Parameter Tunning
[ 2023-03-18 13:40:22,948 ] 57 root - INFO - Real Train Data [ 73.  58.  55.  37.  74.  79.  53.  75.  47.  66.  53.  66.  38.  64.
  57.  85.  90.  68.  76.  64.  62.  62.  76.  63.  54.  81.  66.  62.
  67.  74.  77.  85.  55.  52.  62.  74.  38.  48.  92.  76.  50.  48.
  50.  75.  45.  75.  60.  73.  81.  79.  75.  57.  79.  57.  68.  70.
  59.  47.  59.  68.  65.  58.  86.  76.  57.  67.  80.  61.  62.  65.
  39.  81.  81.  79.  91.  74.  75.  61.  54.  52.  66.  81.  97.  53.
  74.  57.  38.  69.  69.  84.  91.  36.  72.  79.  53.  65.  83.  87.
  47.  59.  68.  52.  58.  68.  64.  35.  69.  73.  79.  65.  69.  29.
  65.  80.  42.  44.  65.  73.  53.  65.  72.  80.  71.  63.  53.  83.
  71.  62.  59.  72.  62.  46.  67.  85.  91.  59.  76.  66.  59.  79.
  98.  27.  73.  88.  73.  68.  69.  72.  62.  86.  69.  82.  87.  58.
  89.  84.  35.  77.  51.  63.  84.  64.  54.  42.  56.  94.  80.  70.
  49.  69.   8.  58.  73.  61.  47.  59.  65.  26.  64.  59.  76.  82.
  77.  91.  69.  74.  62.  69.  73.  69.  62.  80.  48.  86.  43.  50.
  49.  65.  55.  51.  73.  52.  43.  75.  44.  81.  64.  65.  71.  65.
  30.  83.  85.  66.  76.  58.  92.  37.  75.  60.  55.  66.  76.  96.
  50.  49.  83.  92.  96.  73.  82.  67.  67.  48.  22.  70.  77.  56.
  53.  65.  58.  89.  66.  74.  83.  62.  73.  81.  47.  87.  80.  62.
  53. 100.  63.   0.  85.  68.  61.  34.  58.  78.  70.  61.  79.  68.
  70.  51.  73.  74.  82.  62.  19.  67.  70.  87.  63.  65.  61.  62.
  65.  49.  54.  46.  50.  45.  86.  67.  64.  55.  64.  71.  80.  71.
  66.  82.  50.  33.  40.  70.  63.  86.  48.  92.  59.  65.  62.  93.
  53.  58.  71.  77.  74.  78.  71.  46.  53.  62.  64.  62.  88.  62.
  69.  90.  53.  88.  69.  81.  65.  61.  99.  71.  62.  52.  79.  90.
  72.  61.  63.  89.  35.  54.  74.  78.  76.  53.  68.  75.  67.  69.
  81.  63.  41.  57.  61.  84.  61.  59.  49.  49.  41.  58.  76.  76.
  53.  49.  48.  55.  32.  61.  68.  72.  44.  63.  60.  90.  77.  61.
  58.  49.  64.  98.  55.  94.  71.  53.  63.  55.  46.  63.  76.  58.
  90.  72.  76.  32.  64.  36.  61.  81.  55.  85.  60.  60.  59.  80.
  82.  50.  35.  52.  96.  77.  82.  71.  83.  85.  69.  62.  69.  44.
  50.  54.  67.  59.  47.  54.  80.  61.  64.  49.  62.  44.  89.  81.
  68.  77.  65.  77.  88.  83.  73.  69.  68.  69.  58.  46.  61.  57.
  49.  61.  49.  56.  75.  58.  63.  78.  41.  46.  54.  73.  64. 100.
  57.  50.  67.  49.  70.  84.  42.  75.  61.  59.  99.  78.  78.  40.
  74.  74.  82.  62.  58.  75.  63.  34.  69.  52.  63.  69. 100.  75.
  68.  58.  65.  94.  76.  84.  65.  72.  63.  76.  69.  78.  81. 100.
  70.  44.  30.  84.  68.  84.  50.  81.  70.  84.  59.  98.  67.  73.
  88.  50.  77.  40.  75.  77.  54.  90.  64.  41.  49.  85.  77.  57.
  39.  94.  71. 100.  71.  58.  45.  67.  81.  82.  71.  70.  51.  66.
  79.  50.  72.  63.  84.  59.  91.  47.  97.  65.  58.  63.  54.  60.
  39.  68.  58.  57.  82.  91.  65.  57.  99.  56.  70.  59.  52.  82.
  53.  70.  67.  71.  50.  74.  66.  47.  59.  66.  52.  81.  75.  90.
  69.  82.  65.  62.  62.  62.  28.  61.  40.  52.  72.  45.  68.  94.
  44.  55.  74.  71.  79.  80.  46.  41.  52.  48.  77.  55.  45.  65.
  71.  65.  49.  45.  75.  69.  60.  81.  42.  68.  75.  62.  64.  82.
  74.  66.  81.  40.  60.  75.  49.  61.  87.  79.  87.  53.  77.  59.
  76.  52.  95.  91.  54.  55.  65.  93.  53.  67.  53.  68.  71.  41.
  57.  45.  88.  60.  81.  77.  66.  49.  53.  61.  47.  89.  71.  94.
  59.  59.  58.  85.  74.  65.  54.  59.  66.  77.  71.  93.  55.  52.
  73.  71.  60.  69.  40.  76.  52.  47.  77.  45.  63.  51.  48.  60.
  59.  72.  55.  76.  73.  72.  56.  82.  63.  67.  62.  87.  44.  55.
  66.  63.  85.  57.  60.  88.  53.  87.  64.  77.  67.  65.  50.  40.
  57.  57.  23.  67.  79.  81.  77.  70.  78.  51.  54.  63.  35.  64.
  67.  92.  65.  46.  46.  37.  80.  42.  68.  58.  45.  75.  80.  49.
  74.  67.  61.  63.  77.  60.  56.  77.  60.  73.  73.  78.  73.  73.
  87.  62.  57.  46.  74.  92.  61.  81.  73.  72.  66.  73.  32.  82.
  68.  97.  51.  59. 100.  87.  62.  69.  51.  88.  58.  59.  61.  62.
  67.  73.] and Predicted Train Data [52. 49. 71. 58. 98. 71. 78. 68. 84. 49. 40. 84. 62. 77. 50. 53. 61. 63.
 73. 61. 78. 52. 56. 55. 77. 68. 69. 84. 79. 53. 61. 77. 79. 88. 81. 62.
 64. 56. 69. 32. 57. 58. 52. 82. 74. 52. 66. 88. 71. 52. 73. 94. 40. 91.
 88. 68. 55. 91. 76. 62. 58. 70. 70. 61. 53. 73. 71. 45. 87. 53. 44. 67.
 44. 63. 64. 87. 92. 77. 70. 92. 82. 80. 32. 83. 53. 59. 94. 84. 62. 81.
 81. 81. 55. 76. 74. 65. 81. 66. 45. 73. 52. 63. 68. 77. 90. 82. 52. 88.
 59. 91.  8. 60. 65. 70. 87. 56. 19. 66. 76. 59. 76. 67. 71. 45. 74. 91.
 68. 67. 54. 73. 53. 48. 57. 62. 76. 89. 44. 71. 70. 71. 72. 97. 54. 93.
 63. 62. 88. 62. 48. 90. 97. 55. 86. 68. 70. 53. 72. 72. 62. 79. 77. 57.
 57. 69. 62. 72. 71. 49. 79. 80. 76. 60. 74. 84. 78. 76. 54. 59. 86. 68.
 88. 61. 71. 72. 85. 46. 57. 80. 68. 60. 58. 67. 40. 84. 73. 65. 74. 81.
 38. 55.]
[ 2023-03-18 13:40:22,951 ] 58 root - INFO - Real Train Data [ 52.  40.  65.  63.  93.  66.  84.  65.  86.  51.  50.  76.  53.  79.
  43.  57.  62.  51.  74.  54.  65.  53.  51.  68.  79.  67.  60.  97.
  68.  59.  67.  85.  71.  88.  85.  74.  62.  55.  74.  37.  68.  53.
  63.  87.  65.  43.  69.  91.  59.  59.  67.  79.  40.  87.  86.  71.
  65.  82.  69.  71.  62.  72.  66.  72.  44.  82.  66.  42.  97.  59.
  55.  71.  59.  64.  55. 100.  88.  62.  64.  88.  95.  75.  29.  88.
  39.  66.  85.  80.  72.  79.  79.  80.  52.  68.  75.  66.  78.  74.
  61.  69.  40.  48.  72.  79.  87.  88.  54.  81.  68.  78.  24.  54.
  63.  69.  88.  54.  18.  67.  61.  61.  87.  65.  76.  48.  65.  82.
  74.  68.  53.  71.  47.  61.  67.  58.  69.  87.  43.  70.  66.  88.
  73.  91.  69.  90.  60.  59.  97.  77.  27.  94.  85.  56.  77.  72.
  56.  64.  80.  81.  79.  70.  79.  73.  56.  66.  59.  78.  63.  58.
  69.  83.  70.  54.  86.  80.  79.  69.  58.  52.  78.  76.  77.  74.
  70.  74.  80.  29.  73.  87.  59.  62.  52.  71.  48.  89.  65.  67.
  75.  78.  46.  57.] and Predicted Train Data [52. 49. 71. 58. 98. 71. 78. 68. 84. 49. 40. 84. 62. 77. 50. 53. 61. 63.
 73. 61. 78. 52. 56. 55. 77. 68. 69. 84. 79. 53. 61. 77. 79. 88. 81. 62.
 64. 56. 69. 32. 57. 58. 52. 82. 74. 52. 66. 88. 71. 52. 73. 94. 40. 91.
 88. 68. 55. 91. 76. 62. 58. 70. 70. 61. 53. 73. 71. 45. 87. 53. 44. 67.
 44. 63. 64. 87. 92. 77. 70. 92. 82. 80. 32. 83. 53. 59. 94. 84. 62. 81.
 81. 81. 55. 76. 74. 65. 81. 66. 45. 73. 52. 63. 68. 77. 90. 82. 52. 88.
 59. 91.  8. 60. 65. 70. 87. 56. 19. 66. 76. 59. 76. 67. 71. 45. 74. 91.
 68. 67. 54. 73. 53. 48. 57. 62. 76. 89. 44. 71. 70. 71. 72. 97. 54. 93.
 63. 62. 88. 62. 48. 90. 97. 55. 86. 68. 70. 53. 72. 72. 62. 79. 77. 57.
 57. 69. 62. 72. 71. 49. 79. 80. 76. 60. 74. 84. 78. 76. 54. 59. 86. 68.
 88. 61. 71. 72. 85. 46. 57. 80. 68. 60. 58. 67. 40. 84. 73. 65. 74. 81.
 38. 55.]
[ 2023-03-18 13:40:22,953 ] 65 root - INFO - Test model score : 0.9991277168639049 
 Test Model Score: 0.7234199427584452
[ 2023-03-18 13:40:22,953 ] 67 root - INFO - Report Addded : {'Random Forest': 0.8448045870912733, 'Decision Tree': 0.7234199427584452}
[ 2023-03-18 13:40:22,953 ] 39 root - INFO - Evaluting Model: GradientBoostingRegressor() on Params; {'loss': ['squared_error', 'huber', 'absolute_error', 'quantile'], 'learning_rate': [0.1, 0.01, 0.05, 0.001], 'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9], 'criterion': ['squared_error', 'friedman_mse'], 'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [8, 16, 32, 64, 128, 256]}
[ 2023-03-18 13:40:22,953 ] 43 root - INFO - GridSearchCV(cv=3, estimator=GradientBoostingRegressor(),
             param_grid={'criterion': ['squared_error', 'friedman_mse'],
                         'learning_rate': [0.1, 0.01, 0.05, 0.001],
                         'loss': ['squared_error', 'huber', 'absolute_error',
                                  'quantile'],
                         'max_features': ['auto', 'sqrt', 'log2'],
                         'n_estimators': [8, 16, 32, 64, 128, 256],
                         'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9]})
