[ 2023-03-17 21:07:34,199 ] 26 root - INFO - Entered the data ingestion method or components
[ 2023-03-17 21:07:34,207 ] 29 root - INFO - Exported Read The Dataset as DataFrame
[ 2023-03-17 21:07:34,217 ] 35 root - INFO - Train Test Split Inititated
[ 2023-03-17 21:07:34,231 ] 41 root - INFO - Ingestion of Data is completed
[ 2023-03-17 21:07:34,244 ] 84 root - INFO - Read train and test data completed
[ 2023-03-17 21:07:34,244 ] 86 root - INFO - Obtaining preprocessing object
[ 2023-03-17 21:07:34,244 ] 43 root - INFO - Categorical columns: ['gender', 'race_ethnicity', 'parental_level_of_education', 'lunch', 'test_preparation_course']
[ 2023-03-17 21:07:34,244 ] 44 root - INFO - Numerical columns: ['writing_score', 'reading_score']
[ 2023-03-17 21:07:34,248 ] 60 root - INFO - Categorical Pipeline: Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),
                ('one_hot_encoder', OneHotEncoder()),
                ('scaler', StandardScaler(with_mean=False))])
[ 2023-03-17 21:07:34,251 ] 61 root - INFO - Numerical Pipline: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler(with_mean=False))])
[ 2023-03-17 21:07:34,251 ] 70 root - INFO - Categorical Pipline &Numerical Pipeline Excueted Scuceesully
[ 2023-03-17 21:07:34,267 ] 71 root - INFO - Preprocesssor is returned ColumnTransformer(transformers=[('num_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='median')),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['writing_score', 'reading_score']),
                                ('cat_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='most_frequent')),
                                                 ('one_hot_encoder',
                                                  OneHotEncoder()),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['gender', 'race_ethnicity',
                                  'parental_level_of_education', 'lunch',
                                  'test_preparation_course'])])
[ 2023-03-17 21:07:34,267 ] 102 root - INFO - Categorical columns: ['gender', 'race_ethnicity', 'parental_level_of_education', 'lunch', 'test_preparation_course']
[ 2023-03-17 21:07:34,267 ] 103 root - INFO - Numerical columns: ['writing_score', 'reading_score']
[ 2023-03-17 21:07:34,267 ] 104 root - INFO - Target columns: math_score
[ 2023-03-17 21:07:34,267 ] 106 root - INFO - Removing Target Columns from Train and Test Set
[ 2023-03-17 21:07:34,270 ] 115 root - INFO - Applying preprocessing object on training dataframe and testing dataframe.
[ 2023-03-17 21:07:34,299 ] 130 root - INFO - Data Transformation Complted.
[ 2023-03-17 21:07:34,311 ] 140 root - INFO - Saved preprocessing object.
[ 2023-03-17 21:07:34,312 ] 39 root - INFO - Split training and test input data
[ 2023-03-17 21:07:37,737 ] 61 root - INFO - Model Report Genrated : {'Radom Forest': 0.8415496016807171, 'Decision Tree': 0.7461829328381173, 'Gradient Boosting': 0.8527169059991049, 'Linear Regression': 0.8704023080124457, 'K-Neighbors Regression': 0.4981420508414579, 'XGBRegression': 0.8399685131498817, 'CatBoosting Regression': 0.8476986775457768, 'AdaBoost Regression': 0.8286793994191535}
[ 2023-03-17 21:07:37,738 ] 69 root - INFO - Best Model Score: 0.8704023080124457 Best Model Name: Linear Regression
[ 2023-03-17 21:07:37,738 ] 77 root - INFO - Best found model on both training and testing dataset
[ 2023-03-17 21:07:37,739 ] 79 root - INFO - Best Model  : LinearRegression()
[ 2023-03-17 21:07:37,741 ] 86 root - INFO - Model Scussecfully Formed
[ 2023-03-17 21:07:37,745 ] 90 root - INFO - Prediction Completed : [ 51.10961914  46.51635742  62.21484375  60.03405762  90.67407227
  64.4119873   86.86999512  61.45068359  82.953125    51.1640625
  52.17346191  82.63891602  56.8848877   76.58947754  50.87316895
  52.84020996  70.87512207  61.14135742  72.07543945  54.72253418
  72.39733887  60.12634277  54.47167969  60.32348633  81.01403809
  67.78625488  62.41638184  94.37182617  69.68029785  49.22827148
  60.04614258  81.95593262  71.68029785  82.1394043   74.82580566
  69.33679199  64.34655762  59.75463867  72.70275879  40.84375
  54.54785156  53.53320312  52.08227539  88.2980957   71.39624023
  48.42614746  70.47583008  95.17199707  60.51806641  53.48425293
  73.91699219  88.76586914  44.20373535  85.81555176  90.05566406
  69.18920898  58.48876953  82.06787109  67.3203125   66.92687988
  57.94506836  72.54248047  71.30737305  68.25195312  52.95227051
  80.49475098  68.2890625   55.08166504  88.93115234  54.64941406
  60.66577148  76.72180176  54.10375977  63.97436523  59.87390137
 105.64697266  86.42468262  64.26330566  60.24560547  87.98510742
  84.32861328  78.83251953  35.21789551  82.75268555  52.04040527
  65.390625    82.97363281  81.95336914  64.72338867  80.51306152
  79.22094727  76.41699219  50.52001953  67.83679199  78.82080078
  67.55444336  85.05737305  73.2331543   52.94238281  67.27026367
  51.98474121  58.62109375  70.27526855  75.30578613  91.21899414
  82.25390625  54.16723633  89.38708496  62.0904541   76.46081543
  22.50744629  54.91564941  59.5703125   77.85986328  78.4005127
  58.36657715  21.53112793  62.74755859  68.56689453  64.03918457
  81.93066406  65.9296875   72.23095703  51.0579834   69.85083008
  84.51196289  74.59033203  65.38391113  58.33605957  72.85583496
  52.39453125  48.4822998   70.55737305  64.0246582   78.48754883
  85.91821289  44.11682129  64.44213867  61.1184082   77.47729492
  71.70361328  97.40551758  62.94995117  91.375       66.79138184
  57.87658691  88.82714844  67.57397461  40.13586426  89.42602539
  91.38989258  53.26086426  79.03173828  68.03735352  61.93212891
  52.11523438  68.82507324  75.64245605  70.83984375  74.4440918
  78.0871582   66.46203613  55.51477051  65.36376953  63.75646973
  73.25598145  66.37597656  56.86755371  70.48693848  79.70629883
  74.9732666   52.45812988  82.7220459   79.60473633  80.3795166
  73.5357666   57.66796875  57.99890137  85.37963867  78.47973633
  79.39916992  75.81604004  64.59094238  67.84448242  78.33422852
  30.72497559  67.19458008  82.47790527  70.65637207  63.77783203
  57.38989258  72.41540527  51.90307617  86.18920898  74.31201172
  65.48522949  76.92041016  78.64953613  44.65026855  52.26269531]
[ 2023-03-17 21:07:37,745 ] 93 root - INFO - R2 Score Genrated <function r2_score at 0x000001FDB349C790>
