[ 2023-03-18 13:32:10,821 ] 26 root - INFO - Entered the data ingestion method or components
[ 2023-03-18 13:32:10,830 ] 29 root - INFO - Exported Read The Dataset as DataFrame
[ 2023-03-18 13:32:10,844 ] 35 root - INFO - Train Test Split Inititated
[ 2023-03-18 13:32:10,864 ] 41 root - INFO - Ingestion of Data is completed
[ 2023-03-18 13:32:10,864 ] 62 root - INFO - Train Data : artifact\train.csv, Test Data : artifact\test.csv
[ 2023-03-18 13:32:10,878 ] 85 root - INFO - Read train and test data completed
[ 2023-03-18 13:32:10,878 ] 87 root - INFO - Obtaining preprocessing object
[ 2023-03-18 13:32:10,878 ] 34 root - INFO - Entered the data transfomation method or components
[ 2023-03-18 13:32:10,878 ] 44 root - INFO - Categorical columns: ['gender', 'race_ethnicity', 'parental_level_of_education', 'lunch', 'test_preparation_course']
[ 2023-03-18 13:32:10,879 ] 45 root - INFO - Numerical columns: ['writing_score', 'reading_score']
[ 2023-03-18 13:32:10,881 ] 61 root - INFO - Categorical Pipeline: Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),
                ('one_hot_encoder', OneHotEncoder()),
                ('scaler', StandardScaler(with_mean=False))])
[ 2023-03-18 13:32:10,883 ] 62 root - INFO - Numerical Pipline: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler(with_mean=False))])
[ 2023-03-18 13:32:10,883 ] 71 root - INFO - Categorical Pipline &Numerical Pipeline Excueted Scuceesully
[ 2023-03-18 13:32:10,909 ] 72 root - INFO - Preprocesssor is returned ColumnTransformer(transformers=[('num_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='median')),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['writing_score', 'reading_score']),
                                ('cat_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='most_frequent')),
                                                 ('one_hot_encoder',
                                                  OneHotEncoder()),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['gender', 'race_ethnicity',
                                  'parental_level_of_education', 'lunch',
                                  'test_preparation_course'])])
[ 2023-03-18 13:32:10,910 ] 103 root - INFO - Categorical columns: ['gender', 'race_ethnicity', 'parental_level_of_education', 'lunch', 'test_preparation_course']
[ 2023-03-18 13:32:10,910 ] 104 root - INFO - Numerical columns: ['writing_score', 'reading_score']
[ 2023-03-18 13:32:10,910 ] 105 root - INFO - Target columns: math_score
[ 2023-03-18 13:32:10,910 ] 107 root - INFO - Removing Target Columns from Train and Test Set
[ 2023-03-18 13:32:10,912 ] 116 root - INFO - Applying preprocessing object on training dataframe and testing dataframe.
[ 2023-03-18 13:32:10,937 ] 131 root - INFO - Data Transformation Complted.
[ 2023-03-18 13:32:10,937 ] 15 root - INFO - Saving object {obj} on file: {file_path} Started.
[ 2023-03-18 13:32:10,937 ] 18 root - INFO - Making Directory :artifact
[ 2023-03-18 13:32:10,937 ] 20 root - INFO - Directory Made
[ 2023-03-18 13:32:10,953 ] 21 root - INFO - Dumping object : ColumnTransformer(transformers=[('num_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='median')),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['writing_score', 'reading_score']),
                                ('cat_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='most_frequent')),
                                                 ('one_hot_encoder',
                                                  OneHotEncoder()),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['gender', 'race_ethnicity',
                                  'parental_level_of_education', 'lunch',
                                  'test_preparation_course'])])
[ 2023-03-18 13:32:10,972 ] 25 root - INFO - Dumping of obj ColumnTransformer(transformers=[('num_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='median')),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['writing_score', 'reading_score']),
                                ('cat_pipeline',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='most_frequent')),
                                                 ('one_hot_encoder',
                                                  OneHotEncoder()),
                                                 ('scaler',
                                                  StandardScaler(with_mean=False))]),
                                 ['gender', 'race_ethnicity',
                                  'parental_level_of_education', 'lunch',
                                  'test_preparation_course'])]) Successfull <_io.BufferedWriter name='artifact\\preprocessor.pkl'>
[ 2023-03-18 13:32:10,972 ] 141 root - INFO - Saved preprocessing object as Pickle File.
[ 2023-03-18 13:32:10,974 ] 66 root - INFO - Train Array : [[ 4.30533129  4.62062258  0.         ...  0.          2.09830697
  73.        ]
 [ 4.37056358  4.75652325  2.0025047  ...  0.          2.09830697
  58.        ]
 [ 2.80498857  3.12571528  0.         ...  0.          2.09830697
  55.        ]
 ...
 [ 4.56626046  4.96037424  2.0025047  ...  0.          2.09830697
  62.        ]
 [ 5.47951255  5.36807623  2.0025047  ...  2.09830697  0.
  67.        ]
 [ 3.71824066  4.34882125  0.         ...  0.          2.09830697
  73.        ]],Test Array : [[ 3.00068544  3.32956627  0.         ...  2.09830697  0.
  52.        ]
 [ 3.32684691  4.00906959  2.0025047  ...  0.          2.09830697
  40.        ]
 [ 4.56626046  4.68857291  2.0025047  ...  0.          2.09830697
  65.        ]
 ...
 [ 5.21858338  5.23217557  0.         ...  2.09830697  0.
  78.        ]
 [ 3.78347295  3.66931793  2.0025047  ...  2.09830697  0.
  46.        ]
 [ 4.1748667   3.94111926  2.0025047  ...  2.09830697  0.
  57.        ]],Preprocessor Array : artifact\preprocessor.pkl
[ 2023-03-18 13:32:10,974 ] 38 root - INFO - Entered the Model Traineer method or components
[ 2023-03-18 13:32:10,974 ] 40 root - INFO - Split training and test input data
[ 2023-03-18 13:32:10,974 ] 31 root - INFO - Evaluting Model Started
[ 2023-03-18 13:32:10,974 ] 39 root - INFO - Evaluting Model: RandomForestRegressor() on Params; {'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], 'max_features': ['sqrt', 'log2', None], 'n_estimators': [8, 16, 32, 64, 128, 256]}
[ 2023-03-18 13:32:10,974 ] 43 root - INFO - GridSearchCV(cv=3, estimator=RandomForestRegressor(),
             param_grid={'criterion': ['squared_error', 'friedman_mse',
                                       'absolute_error', 'poisson'],
                         'max_features': ['sqrt', 'log2', None],
                         'n_estimators': [8, 16, 32, 64, 128, 256]})
[ 2023-03-18 13:34:08,123 ] 45 root - INFO - Doing Grid Search CV
[ 2023-03-18 13:34:12,043 ] 50 root - INFO - Model Trained With Hyper Parameter Tunning
[ 2023-03-18 13:34:12,094 ] 57 root - INFO - Real Train Data [ 73.  58.  55.  37.  74.  79.  53.  75.  47.  66.  53.  66.  38.  64.
  57.  85.  90.  68.  76.  64.  62.  62.  76.  63.  54.  81.  66.  62.
  67.  74.  77.  85.  55.  52.  62.  74.  38.  48.  92.  76.  50.  48.
  50.  75.  45.  75.  60.  73.  81.  79.  75.  57.  79.  57.  68.  70.
  59.  47.  59.  68.  65.  58.  86.  76.  57.  67.  80.  61.  62.  65.
  39.  81.  81.  79.  91.  74.  75.  61.  54.  52.  66.  81.  97.  53.
  74.  57.  38.  69.  69.  84.  91.  36.  72.  79.  53.  65.  83.  87.
  47.  59.  68.  52.  58.  68.  64.  35.  69.  73.  79.  65.  69.  29.
  65.  80.  42.  44.  65.  73.  53.  65.  72.  80.  71.  63.  53.  83.
  71.  62.  59.  72.  62.  46.  67.  85.  91.  59.  76.  66.  59.  79.
  98.  27.  73.  88.  73.  68.  69.  72.  62.  86.  69.  82.  87.  58.
  89.  84.  35.  77.  51.  63.  84.  64.  54.  42.  56.  94.  80.  70.
  49.  69.   8.  58.  73.  61.  47.  59.  65.  26.  64.  59.  76.  82.
  77.  91.  69.  74.  62.  69.  73.  69.  62.  80.  48.  86.  43.  50.
  49.  65.  55.  51.  73.  52.  43.  75.  44.  81.  64.  65.  71.  65.
  30.  83.  85.  66.  76.  58.  92.  37.  75.  60.  55.  66.  76.  96.
  50.  49.  83.  92.  96.  73.  82.  67.  67.  48.  22.  70.  77.  56.
  53.  65.  58.  89.  66.  74.  83.  62.  73.  81.  47.  87.  80.  62.
  53. 100.  63.   0.  85.  68.  61.  34.  58.  78.  70.  61.  79.  68.
  70.  51.  73.  74.  82.  62.  19.  67.  70.  87.  63.  65.  61.  62.
  65.  49.  54.  46.  50.  45.  86.  67.  64.  55.  64.  71.  80.  71.
  66.  82.  50.  33.  40.  70.  63.  86.  48.  92.  59.  65.  62.  93.
  53.  58.  71.  77.  74.  78.  71.  46.  53.  62.  64.  62.  88.  62.
  69.  90.  53.  88.  69.  81.  65.  61.  99.  71.  62.  52.  79.  90.
  72.  61.  63.  89.  35.  54.  74.  78.  76.  53.  68.  75.  67.  69.
  81.  63.  41.  57.  61.  84.  61.  59.  49.  49.  41.  58.  76.  76.
  53.  49.  48.  55.  32.  61.  68.  72.  44.  63.  60.  90.  77.  61.
  58.  49.  64.  98.  55.  94.  71.  53.  63.  55.  46.  63.  76.  58.
  90.  72.  76.  32.  64.  36.  61.  81.  55.  85.  60.  60.  59.  80.
  82.  50.  35.  52.  96.  77.  82.  71.  83.  85.  69.  62.  69.  44.
  50.  54.  67.  59.  47.  54.  80.  61.  64.  49.  62.  44.  89.  81.
  68.  77.  65.  77.  88.  83.  73.  69.  68.  69.  58.  46.  61.  57.
  49.  61.  49.  56.  75.  58.  63.  78.  41.  46.  54.  73.  64. 100.
  57.  50.  67.  49.  70.  84.  42.  75.  61.  59.  99.  78.  78.  40.
  74.  74.  82.  62.  58.  75.  63.  34.  69.  52.  63.  69. 100.  75.
  68.  58.  65.  94.  76.  84.  65.  72.  63.  76.  69.  78.  81. 100.
  70.  44.  30.  84.  68.  84.  50.  81.  70.  84.  59.  98.  67.  73.
  88.  50.  77.  40.  75.  77.  54.  90.  64.  41.  49.  85.  77.  57.
  39.  94.  71. 100.  71.  58.  45.  67.  81.  82.  71.  70.  51.  66.
  79.  50.  72.  63.  84.  59.  91.  47.  97.  65.  58.  63.  54.  60.
  39.  68.  58.  57.  82.  91.  65.  57.  99.  56.  70.  59.  52.  82.
  53.  70.  67.  71.  50.  74.  66.  47.  59.  66.  52.  81.  75.  90.
  69.  82.  65.  62.  62.  62.  28.  61.  40.  52.  72.  45.  68.  94.
  44.  55.  74.  71.  79.  80.  46.  41.  52.  48.  77.  55.  45.  65.
  71.  65.  49.  45.  75.  69.  60.  81.  42.  68.  75.  62.  64.  82.
  74.  66.  81.  40.  60.  75.  49.  61.  87.  79.  87.  53.  77.  59.
  76.  52.  95.  91.  54.  55.  65.  93.  53.  67.  53.  68.  71.  41.
  57.  45.  88.  60.  81.  77.  66.  49.  53.  61.  47.  89.  71.  94.
  59.  59.  58.  85.  74.  65.  54.  59.  66.  77.  71.  93.  55.  52.
  73.  71.  60.  69.  40.  76.  52.  47.  77.  45.  63.  51.  48.  60.
  59.  72.  55.  76.  73.  72.  56.  82.  63.  67.  62.  87.  44.  55.
  66.  63.  85.  57.  60.  88.  53.  87.  64.  77.  67.  65.  50.  40.
  57.  57.  23.  67.  79.  81.  77.  70.  78.  51.  54.  63.  35.  64.
  67.  92.  65.  46.  46.  37.  80.  42.  68.  58.  45.  75.  80.  49.
  74.  67.  61.  63.  77.  60.  56.  77.  60.  73.  73.  78.  73.  73.
  87.  62.  57.  46.  74.  92.  61.  81.  73.  72.  66.  73.  32.  82.
  68.  97.  51.  59. 100.  87.  62.  69.  51.  88.  58.  59.  61.  62.
  67.  73.] and Predicted Train Data [53.3671875  46.859375   62.41015625 58.21875    91.28125    62.1484375
 92.13671875 59.92578125 83.03125    49.0078125  53.09375    83.4453125
 60.046875   75.1328125  48.328125   50.66796875 68.3515625  61.6171875
 72.69140625 57.765625   70.94921875 59.3359375  59.3515625  58.09375
 79.7109375  70.8125     59.9609375  93.07421875 70.0546875  50.8203125
 57.4375     83.375      68.734375   85.328125   73.9296875  68.6484375
 60.296875   62.0859375  69.484375   39.609375   58.1953125  51.109375
 50.92578125 84.2421875  66.3203125  48.7890625  67.4921875  95.6015625
 59.1796875  50.1796875  73.015625   90.0859375  44.140625   84.859375
 86.0546875  72.23828125 56.265625   87.3046875  72.3203125  65.578125
 56.359375   71.609375   72.65625    67.6796875  50.953125   78.96875
 68.828125   54.0390625  93.01171875 50.640625   60.7421875  79.3984375
 52.7734375  63.2421875  61.4921875  96.40234375 84.5859375  67.1796875
 62.421875   91.8828125  83.49609375 76.296875   41.5        83.2265625
 51.078125   59.2109375  87.265625   79.4296875  64.875      79.140625
 81.6875     75.953125   52.609375   69.9140625  78.140625   66.9609375
 83.6484375  66.3828125  50.1171875  65.171875   51.578125   55.703125
 69.7265625  76.6796875  93.203125   83.9140625  54.359375   84.1328125
 62.625      83.6640625  21.890625   56.1640625  56.1015625  75.8125
 78.2734375  55.1328125  24.9453125  65.1640625  71.34375    63.078125
 85.0546875  65.6796875  73.09375    51.8984375  69.7265625  85.484375
 78.3359375  62.44140625 59.6328125  71.9375     50.5859375  47.09375
 66.84375    60.8828125  79.96875    88.5625     46.8828125  62.671875
 61.46875    77.25       68.1640625  95.34375    65.9453125  92.08203125
 65.453125   55.28125    89.734375   63.15625    45.890625   90.9296875
 89.5859375  53.3125     78.8515625  71.953125   60.8046875  52.671875
 69.859375   72.109375   70.7578125  75.515625   80.5390625  69.703125
 58.3671875  64.2109375  62.4765625  73.921875   66.703125   57.71875
 70.828125   75.375      75.0078125  55.0390625  80.765625   81.59375
 80.84375    73.34375    53.8125     57.1640625  83.84375    75.328125
 89.7890625  71.3125     64.8203125  64.578125   78.6015625  33.8046875
 64.58984375 86.59375    71.9375     62.5234375  53.40625    65.3125
 52.71875    91.09765625 76.859375   62.9609375  77.1640625  80.125
 49.0625     55.4140625 ]
[ 2023-03-18 13:34:12,098 ] 58 root - INFO - Real Train Data [ 52.  40.  65.  63.  93.  66.  84.  65.  86.  51.  50.  76.  53.  79.
  43.  57.  62.  51.  74.  54.  65.  53.  51.  68.  79.  67.  60.  97.
  68.  59.  67.  85.  71.  88.  85.  74.  62.  55.  74.  37.  68.  53.
  63.  87.  65.  43.  69.  91.  59.  59.  67.  79.  40.  87.  86.  71.
  65.  82.  69.  71.  62.  72.  66.  72.  44.  82.  66.  42.  97.  59.
  55.  71.  59.  64.  55. 100.  88.  62.  64.  88.  95.  75.  29.  88.
  39.  66.  85.  80.  72.  79.  79.  80.  52.  68.  75.  66.  78.  74.
  61.  69.  40.  48.  72.  79.  87.  88.  54.  81.  68.  78.  24.  54.
  63.  69.  88.  54.  18.  67.  61.  61.  87.  65.  76.  48.  65.  82.
  74.  68.  53.  71.  47.  61.  67.  58.  69.  87.  43.  70.  66.  88.
  73.  91.  69.  90.  60.  59.  97.  77.  27.  94.  85.  56.  77.  72.
  56.  64.  80.  81.  79.  70.  79.  73.  56.  66.  59.  78.  63.  58.
  69.  83.  70.  54.  86.  80.  79.  69.  58.  52.  78.  76.  77.  74.
  70.  74.  80.  29.  73.  87.  59.  62.  52.  71.  48.  89.  65.  67.
  75.  78.  46.  57.] and Predicted Train Data [53.3671875  46.859375   62.41015625 58.21875    91.28125    62.1484375
 92.13671875 59.92578125 83.03125    49.0078125  53.09375    83.4453125
 60.046875   75.1328125  48.328125   50.66796875 68.3515625  61.6171875
 72.69140625 57.765625   70.94921875 59.3359375  59.3515625  58.09375
 79.7109375  70.8125     59.9609375  93.07421875 70.0546875  50.8203125
 57.4375     83.375      68.734375   85.328125   73.9296875  68.6484375
 60.296875   62.0859375  69.484375   39.609375   58.1953125  51.109375
 50.92578125 84.2421875  66.3203125  48.7890625  67.4921875  95.6015625
 59.1796875  50.1796875  73.015625   90.0859375  44.140625   84.859375
 86.0546875  72.23828125 56.265625   87.3046875  72.3203125  65.578125
 56.359375   71.609375   72.65625    67.6796875  50.953125   78.96875
 68.828125   54.0390625  93.01171875 50.640625   60.7421875  79.3984375
 52.7734375  63.2421875  61.4921875  96.40234375 84.5859375  67.1796875
 62.421875   91.8828125  83.49609375 76.296875   41.5        83.2265625
 51.078125   59.2109375  87.265625   79.4296875  64.875      79.140625
 81.6875     75.953125   52.609375   69.9140625  78.140625   66.9609375
 83.6484375  66.3828125  50.1171875  65.171875   51.578125   55.703125
 69.7265625  76.6796875  93.203125   83.9140625  54.359375   84.1328125
 62.625      83.6640625  21.890625   56.1640625  56.1015625  75.8125
 78.2734375  55.1328125  24.9453125  65.1640625  71.34375    63.078125
 85.0546875  65.6796875  73.09375    51.8984375  69.7265625  85.484375
 78.3359375  62.44140625 59.6328125  71.9375     50.5859375  47.09375
 66.84375    60.8828125  79.96875    88.5625     46.8828125  62.671875
 61.46875    77.25       68.1640625  95.34375    65.9453125  92.08203125
 65.453125   55.28125    89.734375   63.15625    45.890625   90.9296875
 89.5859375  53.3125     78.8515625  71.953125   60.8046875  52.671875
 69.859375   72.109375   70.7578125  75.515625   80.5390625  69.703125
 58.3671875  64.2109375  62.4765625  73.921875   66.703125   57.71875
 70.828125   75.375      75.0078125  55.0390625  80.765625   81.59375
 80.84375    73.34375    53.8125     57.1640625  83.84375    75.328125
 89.7890625  71.3125     64.8203125  64.578125   78.6015625  33.8046875
 64.58984375 86.59375    71.9375     62.5234375  53.40625    65.3125
 52.71875    91.09765625 76.859375   62.9609375  77.1640625  80.125
 49.0625     55.4140625 ]
[ 2023-03-18 13:34:12,101 ] 65 root - INFO - Test model score : 0.9750895722746692 
 Test Model Score: 0.8427426559365919
[ 2023-03-18 13:34:12,101 ] 67 root - INFO - Report Addded : {'Random Forest': 0.8427426559365919}
[ 2023-03-18 13:34:12,103 ] 39 root - INFO - Evaluting Model: DecisionTreeRegressor() on Params; {'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], 'splitter': ['best', 'random'], 'max_features': ['sqrt', 'log2']}
[ 2023-03-18 13:34:12,104 ] 43 root - INFO - GridSearchCV(cv=3, estimator=DecisionTreeRegressor(),
             param_grid={'criterion': ['squared_error', 'friedman_mse',
                                       'absolute_error', 'poisson'],
                         'max_features': ['sqrt', 'log2'],
                         'splitter': ['best', 'random']})
[ 2023-03-18 13:34:12,339 ] 45 root - INFO - Doing Grid Search CV
[ 2023-03-18 13:34:12,341 ] 50 root - INFO - Model Trained With Hyper Parameter Tunning
[ 2023-03-18 13:34:12,349 ] 57 root - INFO - Real Train Data [ 73.  58.  55.  37.  74.  79.  53.  75.  47.  66.  53.  66.  38.  64.
  57.  85.  90.  68.  76.  64.  62.  62.  76.  63.  54.  81.  66.  62.
  67.  74.  77.  85.  55.  52.  62.  74.  38.  48.  92.  76.  50.  48.
  50.  75.  45.  75.  60.  73.  81.  79.  75.  57.  79.  57.  68.  70.
  59.  47.  59.  68.  65.  58.  86.  76.  57.  67.  80.  61.  62.  65.
  39.  81.  81.  79.  91.  74.  75.  61.  54.  52.  66.  81.  97.  53.
  74.  57.  38.  69.  69.  84.  91.  36.  72.  79.  53.  65.  83.  87.
  47.  59.  68.  52.  58.  68.  64.  35.  69.  73.  79.  65.  69.  29.
  65.  80.  42.  44.  65.  73.  53.  65.  72.  80.  71.  63.  53.  83.
  71.  62.  59.  72.  62.  46.  67.  85.  91.  59.  76.  66.  59.  79.
  98.  27.  73.  88.  73.  68.  69.  72.  62.  86.  69.  82.  87.  58.
  89.  84.  35.  77.  51.  63.  84.  64.  54.  42.  56.  94.  80.  70.
  49.  69.   8.  58.  73.  61.  47.  59.  65.  26.  64.  59.  76.  82.
  77.  91.  69.  74.  62.  69.  73.  69.  62.  80.  48.  86.  43.  50.
  49.  65.  55.  51.  73.  52.  43.  75.  44.  81.  64.  65.  71.  65.
  30.  83.  85.  66.  76.  58.  92.  37.  75.  60.  55.  66.  76.  96.
  50.  49.  83.  92.  96.  73.  82.  67.  67.  48.  22.  70.  77.  56.
  53.  65.  58.  89.  66.  74.  83.  62.  73.  81.  47.  87.  80.  62.
  53. 100.  63.   0.  85.  68.  61.  34.  58.  78.  70.  61.  79.  68.
  70.  51.  73.  74.  82.  62.  19.  67.  70.  87.  63.  65.  61.  62.
  65.  49.  54.  46.  50.  45.  86.  67.  64.  55.  64.  71.  80.  71.
  66.  82.  50.  33.  40.  70.  63.  86.  48.  92.  59.  65.  62.  93.
  53.  58.  71.  77.  74.  78.  71.  46.  53.  62.  64.  62.  88.  62.
  69.  90.  53.  88.  69.  81.  65.  61.  99.  71.  62.  52.  79.  90.
  72.  61.  63.  89.  35.  54.  74.  78.  76.  53.  68.  75.  67.  69.
  81.  63.  41.  57.  61.  84.  61.  59.  49.  49.  41.  58.  76.  76.
  53.  49.  48.  55.  32.  61.  68.  72.  44.  63.  60.  90.  77.  61.
  58.  49.  64.  98.  55.  94.  71.  53.  63.  55.  46.  63.  76.  58.
  90.  72.  76.  32.  64.  36.  61.  81.  55.  85.  60.  60.  59.  80.
  82.  50.  35.  52.  96.  77.  82.  71.  83.  85.  69.  62.  69.  44.
  50.  54.  67.  59.  47.  54.  80.  61.  64.  49.  62.  44.  89.  81.
  68.  77.  65.  77.  88.  83.  73.  69.  68.  69.  58.  46.  61.  57.
  49.  61.  49.  56.  75.  58.  63.  78.  41.  46.  54.  73.  64. 100.
  57.  50.  67.  49.  70.  84.  42.  75.  61.  59.  99.  78.  78.  40.
  74.  74.  82.  62.  58.  75.  63.  34.  69.  52.  63.  69. 100.  75.
  68.  58.  65.  94.  76.  84.  65.  72.  63.  76.  69.  78.  81. 100.
  70.  44.  30.  84.  68.  84.  50.  81.  70.  84.  59.  98.  67.  73.
  88.  50.  77.  40.  75.  77.  54.  90.  64.  41.  49.  85.  77.  57.
  39.  94.  71. 100.  71.  58.  45.  67.  81.  82.  71.  70.  51.  66.
  79.  50.  72.  63.  84.  59.  91.  47.  97.  65.  58.  63.  54.  60.
  39.  68.  58.  57.  82.  91.  65.  57.  99.  56.  70.  59.  52.  82.
  53.  70.  67.  71.  50.  74.  66.  47.  59.  66.  52.  81.  75.  90.
  69.  82.  65.  62.  62.  62.  28.  61.  40.  52.  72.  45.  68.  94.
  44.  55.  74.  71.  79.  80.  46.  41.  52.  48.  77.  55.  45.  65.
  71.  65.  49.  45.  75.  69.  60.  81.  42.  68.  75.  62.  64.  82.
  74.  66.  81.  40.  60.  75.  49.  61.  87.  79.  87.  53.  77.  59.
  76.  52.  95.  91.  54.  55.  65.  93.  53.  67.  53.  68.  71.  41.
  57.  45.  88.  60.  81.  77.  66.  49.  53.  61.  47.  89.  71.  94.
  59.  59.  58.  85.  74.  65.  54.  59.  66.  77.  71.  93.  55.  52.
  73.  71.  60.  69.  40.  76.  52.  47.  77.  45.  63.  51.  48.  60.
  59.  72.  55.  76.  73.  72.  56.  82.  63.  67.  62.  87.  44.  55.
  66.  63.  85.  57.  60.  88.  53.  87.  64.  77.  67.  65.  50.  40.
  57.  57.  23.  67.  79.  81.  77.  70.  78.  51.  54.  63.  35.  64.
  67.  92.  65.  46.  46.  37.  80.  42.  68.  58.  45.  75.  80.  49.
  74.  67.  61.  63.  77.  60.  56.  77.  60.  73.  73.  78.  73.  73.
  87.  62.  57.  46.  74.  92.  61.  81.  73.  72.  66.  73.  32.  82.
  68.  97.  51.  59. 100.  87.  62.  69.  51.  88.  58.  59.  61.  62.
  67.  73.] and Predicted Train Data [60.  49.  59.  58.  98.  67.  69.  55.  91.  50.  40.  85.  62.  77.
 44.  45.  79.  57.  78.  49.  63.  64.  45.  52.  65.  68.  71.  92.
 54.  53.  63.  85.  79.  87.  81.  66.  82.  62.  69.  41.  82.  58.
 49.  81.  67.  47.  73.  91.  70.  52.  65.  94.  45.  94.  92.  74.
 56.  82.  67.  56.  50.  76.  76.  79.  45.  80.  67.  45.  87.  45.
 69.  81.  46.  63.  60.  87.  80.  63.  67.  93.  79.  80.  39.  88.
 64.  63.  84.  74.  62.  85.  75.  81.  52.  76.  72.  65.  85.  67.
 53.  62.  53.  58.  63.  87.  89.  85.  70.  88.  70.  86.  13.5 44.
 61.  70.  66.  64.  13.5 67.  76.  60.  81.  67.  74.  54.  68.  69.
 79.  67.  64.  77.  46.  48.  65.  63.  70.  89.  48.  59.  59.  74.
 71.  75.  66.  93.  63.  49.  94.  69.  65.  81.  88.  47.  86.  75.
 69.  55.  72.  72.  63.  61.  79.  80.  82.  69.  62.  69.  67.  58.
 54.  84.  76.  52.  81.  68.  87.  76.  52.  52.  87.  68.  85.  69.
 71.  72.  81.  50.  57.  82.  55.  63.  50.  67.  53.  94.  59.  65.
 86.  69.  40.  69. ]
[ 2023-03-18 13:34:12,352 ] 58 root - INFO - Real Train Data [ 52.  40.  65.  63.  93.  66.  84.  65.  86.  51.  50.  76.  53.  79.
  43.  57.  62.  51.  74.  54.  65.  53.  51.  68.  79.  67.  60.  97.
  68.  59.  67.  85.  71.  88.  85.  74.  62.  55.  74.  37.  68.  53.
  63.  87.  65.  43.  69.  91.  59.  59.  67.  79.  40.  87.  86.  71.
  65.  82.  69.  71.  62.  72.  66.  72.  44.  82.  66.  42.  97.  59.
  55.  71.  59.  64.  55. 100.  88.  62.  64.  88.  95.  75.  29.  88.
  39.  66.  85.  80.  72.  79.  79.  80.  52.  68.  75.  66.  78.  74.
  61.  69.  40.  48.  72.  79.  87.  88.  54.  81.  68.  78.  24.  54.
  63.  69.  88.  54.  18.  67.  61.  61.  87.  65.  76.  48.  65.  82.
  74.  68.  53.  71.  47.  61.  67.  58.  69.  87.  43.  70.  66.  88.
  73.  91.  69.  90.  60.  59.  97.  77.  27.  94.  85.  56.  77.  72.
  56.  64.  80.  81.  79.  70.  79.  73.  56.  66.  59.  78.  63.  58.
  69.  83.  70.  54.  86.  80.  79.  69.  58.  52.  78.  76.  77.  74.
  70.  74.  80.  29.  73.  87.  59.  62.  52.  71.  48.  89.  65.  67.
  75.  78.  46.  57.] and Predicted Train Data [60.  49.  59.  58.  98.  67.  69.  55.  91.  50.  40.  85.  62.  77.
 44.  45.  79.  57.  78.  49.  63.  64.  45.  52.  65.  68.  71.  92.
 54.  53.  63.  85.  79.  87.  81.  66.  82.  62.  69.  41.  82.  58.
 49.  81.  67.  47.  73.  91.  70.  52.  65.  94.  45.  94.  92.  74.
 56.  82.  67.  56.  50.  76.  76.  79.  45.  80.  67.  45.  87.  45.
 69.  81.  46.  63.  60.  87.  80.  63.  67.  93.  79.  80.  39.  88.
 64.  63.  84.  74.  62.  85.  75.  81.  52.  76.  72.  65.  85.  67.
 53.  62.  53.  58.  63.  87.  89.  85.  70.  88.  70.  86.  13.5 44.
 61.  70.  66.  64.  13.5 67.  76.  60.  81.  67.  74.  54.  68.  69.
 79.  67.  64.  77.  46.  48.  65.  63.  70.  89.  48.  59.  59.  74.
 71.  75.  66.  93.  63.  49.  94.  69.  65.  81.  88.  47.  86.  75.
 69.  55.  72.  72.  63.  61.  79.  80.  82.  69.  62.  69.  67.  58.
 54.  84.  76.  52.  81.  68.  87.  76.  52.  52.  87.  68.  85.  69.
 71.  72.  81.  50.  57.  82.  55.  63.  50.  67.  53.  94.  59.  65.
 86.  69.  40.  69. ]
[ 2023-03-18 13:34:12,353 ] 65 root - INFO - Test model score : 0.9971467374052964 
 Test Model Score: 0.6453420529760094
[ 2023-03-18 13:34:12,353 ] 67 root - INFO - Report Addded : {'Random Forest': 0.8427426559365919, 'Decision Tree': 0.6453420529760094}
[ 2023-03-18 13:34:12,353 ] 39 root - INFO - Evaluting Model: GradientBoostingRegressor() on Params; {'loss': ['squared_error', 'huber', 'absolute_error', 'quantile'], 'learning_rate': [0.1, 0.01, 0.05, 0.001], 'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9], 'criterion': ['squared_error', 'friedman_mse'], 'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [8, 16, 32, 64, 128, 256]}
[ 2023-03-18 13:34:12,354 ] 43 root - INFO - GridSearchCV(cv=3, estimator=GradientBoostingRegressor(),
             param_grid={'criterion': ['squared_error', 'friedman_mse'],
                         'learning_rate': [0.1, 0.01, 0.05, 0.001],
                         'loss': ['squared_error', 'huber', 'absolute_error',
                                  'quantile'],
                         'max_features': ['auto', 'sqrt', 'log2'],
                         'n_estimators': [8, 16, 32, 64, 128, 256],
                         'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9]})
